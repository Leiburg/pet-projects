# Дипломная работа Соколова Александра   
##  DST-10   
### SkillFactory  
![https://img.shields.io/badge/Python-3.8.5-blue](https://img.shields.io/badge/Python-3.8.5-blue)

## Оглавление  
[1. Постановка задачи](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Постановка-задачи)  
[2. Анализ данных](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Предобработка-и-анализ-данных)  
[3. Применение ML и DL](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Применение-ML-и-DL)  
[4. Результаты и выводы](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Результаты-и-выводы)  
[5. Что не успел реализовать](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Что-не-успел-реализовать)  

## Постановка задачи  
В качестве задачи использовалась задача 2 [соревнования AlfaBattle2.0](https://boosters.pro/championship/alfabattle2/overview) (январь 2021 года)  
**Условие задачи:**
Важной составляющей работы банка является ответственное кредитование, основанное на модельной оценке вероятности того, что заёмщик перестанет выполнять взятые обязательства (выйдет в дефолт).  
Необходимо оценить вероятность того, что клиент выйдет в дефолт, основываясь на истории потребительского поведения по карточным транзакциям.  
Каждая такая транзакция содержит информацию о сумме покупки, месте, дате, mcc-категории, валюте и признаки от платежной системы.  
Целевая переменная - бинарная величина, принимающая значения 0 и 1. Функция ошибки для оценки - AUC ROC.  
***Дополнительное исследование по задаче показало:***
- обычно аналогичные модели кредитного скорринга строят на основе данных от бюро кредитных историй (зачастую оценки бюро существенно доминируют по важности над остальными признаками) и использование только транзакционных данных - нетривиальная задача.  
- из-за требований регулятора (ЦБ) в подобных задачах важна интерпретируемость результатов и большинство банков ограничиваются логистической регрессией или решениями построенными на деревьях, так называемые "white-box" решения. Я решил построить модель и интрпретировать ее результаты в виде прототипа модели в виде отдельного web приложения  
- [ребята из сбера опубликовали статью](https://arxiv.org/pdf/1911.02496.pdf) об эффективном применении рекурентных нейронных сетей для решения задачи кредитного скорринга на транзакционных данных и я решил использовать два подхода white-box и black-box, чтобы сравнить их результаты.  
**Цели для дипломной работы:**
- войти в число медалистов соревнования - AUC ROC на привате не ниже 0.7616840  
- реализовать прототип оптимального по размеру и качеству ML-решения в виде web приложения на heroku  
- использовать для оценки моделей метрики f1, AUC PRC, а также матрицы ошибок и другие метрики пытаясь максимально приблизить задачу к реальной, так как метрика AUC ROC не совсем состоятельна в задачах по кредитному скоррингу  
:arrow_up:[к оглавлению](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Оглавление)

## Предобработка и анализ данных
- [описание исходного датасета](https://boosters.pro/championship/alfabattle2_sand/data)  
- сводные характеристики о данных: 1.5 млн клиентов, 450 млн. транзакций, история транзакций глубиной в год , до 8000 транзакций на клиента, тестовая выборка смещена по времени - обучающая выборка собрана за N дней, тестовая выборка за последующие K дней.
- размер исходного датасета в заархивированном виде составил 6Гб. Однако в процессе предобработки данных в агрегационные статистики выяснилось что размер сырых данных сотавляет 25Гб и что локальными ресурсами оперативной памяти (10ГБ) решить данную задачу не получится. Часть предобработки данных была выполненна на ресурсах платформы kaggle, но после того как ресурсов kaggle не хватило для предобработки даты для black-box была использована платформа Google Colab  
- чтобы не перегружать описание диплома я вынес анализ данных (EDA) в отдельный кернел, где представлен полный анализ в том числе с использованием инструментов визуализации  

***Хотелось бы отметить в этом разделе, что предобработка заняла примерно 30-35 %% по времени проекта и даже в какой-то момент казалась невыполнимой, но позволила прокачаться в следующем:***  
- изучение библиотеки [gc (garbage collection)](https://habr.com/ru/post/417215/) позволило более глубоко погрузиться в архитектуру языка python и его реализацию на платформах kaggle (Docker) и Google Colab  
- [типы данных](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage) и размер памяти, который они занимают позволил сократить требуемый размер для хранения датафрейма в двое  
- использование грамотного удаления из памяти датафремов и циклических ссылок на них в совокупности с использованием ресурсов HDD в процессах конкатенации датафреймов, позволил решить задачу на платформах в свободном доступе, хоть и ценой увеличения ресурсов времени и HDD  

**Датасеты с исходными и предобработанными данными:**  
Для удобства воспроизводимости кода решений я создал на платформе kaggle несколько публичных датасетов:
- [датасет](https://www.kaggle.com/sokolovaleks/alfabattle2-sandbox) с исходными данными соревнования и предобработанными данными для бустинга  
- [датасет](https://www.kaggle.com/sokolovaleks/alfabattle2-sandbox) с предобработанными данными для обучения нейронных сетей  
:arrow_up:[к оглавлению](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Оглавление)

## Применение ML и DL
**ML**  
- использовал LightGBM и CatBoost (гиперпараметры можно посмотреть в кернелах)
- предобработка на агрегаты [в этом кернеле](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-1-ml-sokolov) (на случай если вы не хотите добавлять новые агрегатгные функции вы можете переходить к следующему пункту ниже, так как результат препроцессинга был сохранен и используется кернелами ниже)
- [в этом кернеле](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-1-ml-sokolov) реализованы обучение моделей и методы отсева фич такие как: Feature Importance, Permutation importance, Target permutation и методы библиотеки shap
- сравнение и анализ результатов моделей реализован в [отдельном кернеле](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-1-ml-sokolov)
- [прототип на heroku](https://still-garden-79761.herokuapp.com/)  
**DL**  
- пользовал GRU и BIGRU
- препроцессинг реализовал [на Google Colab](https://colab.research.google.com/drive/1Vifze5j9E53MknDc4TxtMmXP6OhIDNps?usp=sharing)

***Дополнительно хотел бы отметить в этом разделе:***
- в работе была использована библиотека shap не рассматриваемая в курсе, но после ознакомления с которой я считаю, что это из разряда "must know" для DS. Она позволяет интерепретировать результаты предсказаний моделей и как выяснилось в том числе black-box моделей.  
:arrow_up:[к оглавлению](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Оглавление)

## Результаты и выводы  
- [презентация](https://docs.google.com/presentation/d/1aGGi9Dmx906WIEpWBeBTaQYEbJ1M6tWu9SPZ4AY5tO4/edit?usp=sharing) 
- [score = 0.7660770](https://boosters.pro/championship/alfabattle2_sand/rating)
- [прототип]() с интерпретацией прогнозов модели  
:arrow_up:[к оглавлению](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Оглавление)

## Что не успел реализовать  

К сожалению не хватило времени, чтобы:
- Разобраться как применять shap для интерпретации модели обученной на нескольких фолдах

- Попробовать вместо блендинга использовать прогнозы нейросети как отдельные признаки и на них построить модели ML  
:arrow_up:[к оглавлению](https://github.com/alex-sokolov2011/skillfactory_rds/blob/master/diplom/README.md#Оглавление)
